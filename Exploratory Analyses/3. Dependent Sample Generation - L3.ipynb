{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Sample Generation\n",
    "In this notebook, we discuss the approach for the calibration of copula models at the L3 level of river basins in Thailand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t, chi2\n",
    "\n",
    "from prisk.analysis_functions import (\n",
    "    combine_glofas, \n",
    "    extract_discharge_timeseries, \n",
    "    fit_gumbel_distribution, \n",
    "    calculate_uniform_marginals)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glofas_dir = \"/Users/rubenkerkhofs/Desktop/glofas/\" \n",
    "basin_outlet_file = \"https://kuleuven-prisk.s3.eu-central-1.amazonaws.com/lev06_outlets_final_clipped_Thailand_no_duplicates.csv\"\n",
    "basin_match_file = \"https://kuleuven-prisk.s3.eu-central-1.amazonaws.com/basin_outlets_match.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we obtain discharge data for the basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load GloFAS river discharge data and upstream accumulating area data\n",
    "# Discharge data for producing GIRI maps is from 1979-2016\n",
    "start_year = 1979\n",
    "end_year = 2016\n",
    "area_filter = 500 # not considering rivers with upstream areas below 500 km^2\n",
    "glofas_data = combine_glofas(start_year, end_year, glofas_dir, area_filter)\n",
    "\n",
    "# Step 2: Load the basin outlet file, perform some data checks (to ensure we have valid discharge timeseries at each basin outlet point), and then extract discharge timeseries for each basin\n",
    "basin_outlets = pd.read_csv(basin_outlet_file)\n",
    "# Note to align the two datasets we need to make the following adjustment to lat lons (based on previous trial and error)\n",
    "basin_outlets['Latitude'] = basin_outlets['Latitude'] + 0.05/2\n",
    "basin_outlets['Longitude'] = basin_outlets['Longitude'] - 0.05/2\n",
    "# Extract discharge timeseries\n",
    "basin_timeseries = extract_discharge_timeseries(basin_outlets, glofas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, the timeseries are obtained, we fit the gumbel distribution to each individual basin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel_params, fit_quality = fit_gumbel_distribution(basin_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Gumbel distributions are fitted, we compute the uniform marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_marginals = calculate_uniform_marginals(basin_timeseries, gumbel_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These uniform marginals are used to estimate the dependency structure between basins. \n",
    "\n",
    "Next, we group these basins using their L3 basin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginals = pd.DataFrame(uniform_marginals)\n",
    "basin_match = pd.read_csv(basin_match_file)\n",
    "l3_basins = basin_match.HYBAS_ID_L3.unique()\n",
    "l3_data = {}\n",
    "\n",
    "for basin in l3_basins:\n",
    "    associated_l6_basins = list(basin_match[basin_match.HYBAS_ID_L3 == basin].HYBAS_ID_L6.unique())\n",
    "    data = marginals[associated_l6_basins]\n",
    "    l3_data[basin] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Copula\n",
    "The Gaussian copula only requires the correlation matrix as an input parameter. We use the GaussianMultivariate object of the copulas package to estimate and sample from this copula. Note that the GaussianMultivariate object also estimates the univariate distributions; however, we have already transformed the univariate distributions to the uniform distribution. For that reason, we fix the uniform distribution.\n",
    "\n",
    "Aseparate gaussian copula is fitted for each of the L3 basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.univariate import UniformUnivariate\n",
    "\n",
    "class UniformUnivariateFixed(UniformUnivariate):\n",
    "    def _fit_constant(self, X):\n",
    "        self._params = {\n",
    "            'loc': 0,\n",
    "            'scale': 1\n",
    "        }\n",
    "\n",
    "    def _fit(self, X):\n",
    "        self._params = {\n",
    "            'loc': 0,\n",
    "            'scale': 1\n",
    "        }\n",
    "\n",
    "copulas = {}\n",
    "for basin, data in l3_data.items():\n",
    "    copula = GaussianMultivariate()\n",
    "    copula.fit(data)\n",
    "    copulas[basin] = copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4448"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.corr().shape[0]*data.corr().shape[1] for data in l3_data.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the sample function can be used to obtain samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    basin: copula.sample(100) for basin, copula in copulas.items()\n",
    "}\n",
    "\n",
    "generated_samples = pd.DataFrame()\n",
    "for basin, sample in samples.items():\n",
    "    generated_samples = pd.concat([generated_samples, sample], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_samples.to_parquet(\"gaussian_random_numbers_L3.parquet.gzip\", compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Copula\n",
    "The T-copula requires two model inputs: (1) the correlation matrix, and (2) the degrees of freedom. In this case, we set the degrees of freedom equal to 3. Samples of the T-Copula are obtained as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "t_samples = {}\n",
    "\n",
    "for basin, data in l3_data.items():\n",
    "    corr_matrix = data.corr().values\n",
    "    mu = np.zeros(len(corr_matrix))\n",
    "    s = chi2.rvs(df=3, size=n_samples)[:, np.newaxis]\n",
    "    Z = np.random.multivariate_normal(mu, corr_matrix, n_samples)\n",
    "    X = np.sqrt(3/s)*Z\n",
    "    U = t.cdf(X, df=3)\n",
    "    t_samples[basin] = pd.DataFrame(U, columns=data.columns)\n",
    "\n",
    "generated_samples = pd.DataFrame()\n",
    "for basin, sample in samples.items():\n",
    "    generated_samples = pd.concat([generated_samples, sample], axis=1)\n",
    "\n",
    "generated_samples.to_parquet(\"t_random_numbers_L3.parquet.gzip\", compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vine Copula\n",
    "The vine copula is estimated using the vinecopulas package. The estimated parameters are pickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vinecopulas.vinecopula import fit_vinecop\n",
    "\n",
    "copula_params = {}\n",
    "\n",
    "for basin, data in l3_data.items():\n",
    "    copula_params[basin] = fit_vinecop(data.values, copsi=list(range(1, 15)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vinecopulas.vinecopula import sample_vinecop\n",
    "\n",
    "samples = {\n",
    "    basin: pd.DataFrame(sample_vinecop(*params, 10000), columns=l3_data[basin].columns) for basin, params in copula_params.items()\n",
    "}\n",
    "\n",
    "generated_samples = pd.DataFrame()\n",
    "for basin, sample in samples.items():\n",
    "    generated_samples = pd.concat([generated_samples, sample], axis=1)\n",
    "\n",
    "#generated_samples.to_parquet(\"vine_random_numbers_L3.parquet.gzip\", compression='gzip', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
